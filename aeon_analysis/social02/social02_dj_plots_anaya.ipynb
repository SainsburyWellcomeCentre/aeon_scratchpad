{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %flow mode reactive\n",
    "\n",
    "from typing import Any, Tuple, List, Dict\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import datajoint as dj\n",
    "from aeon.dj_pipeline.analysis.block_analysis import *\n",
    "from aeon.dj_pipeline import acquisition, streams\n",
    "from aeon.analysis.block_plotting import gen_hex_grad, conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_data(\n",
    "    key: Dict[str, str], \n",
    "    pre_social_start: str, \n",
    "    pre_social_end: str, \n",
    "    social_start: str, \n",
    "    social_end: str, \n",
    "    post_social_start: str, \n",
    "    post_social_end: str\n",
    ") -> Tuple[List[Dict[str, str]], pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loads experiment data for given time periods.\n",
    "\n",
    "    Args:\n",
    "        key (dict): The key to filter the experiment data.\n",
    "        pre_social_start (str): The start time for the pre-social period.\n",
    "        pre_social_end (str): The end time for the pre-social period.\n",
    "        social_start (str): The start time for the social period.\n",
    "        social_end (str): The end time for the social period.\n",
    "        post_social_start (str): The start time for the post-social period.\n",
    "        post_social_end (str): The end time for the post-social period.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - patch_info (list of dict): Information about patches.\n",
    "            - block_subject_patch_data_pre_social (pd.DataFrame): Data for the pre-social period.\n",
    "            - block_subject_patch_data_social (pd.DataFrame): Data for the social period.\n",
    "            - block_subject_patch_data_post_social (pd.DataFrame): Data for the post-social period.\n",
    "    \"\"\"\n",
    "    exp_start = (Block & key & f\"block_start >= '{pre_social_start}'\").fetch(\"block_start\", limit=1)[0]\n",
    "    exp_end = (Block & key & f\"block_start >= '{post_social_end}'\").fetch(\"block_end\", limit=1)[0]\n",
    "    \n",
    "    patch_info = (\n",
    "        BlockAnalysis.Patch\n",
    "        & key\n",
    "        & f'block_start >= \"{exp_start}\"'\n",
    "        & f'block_start <= \"{exp_end}\"'\n",
    "    ).fetch('block_start', \"patch_name\", \"patch_rate\", \"patch_offset\", as_dict=True)\n",
    "\n",
    "    block_subject_patch_data_pre_social = (BlockSubjectAnalysis.Patch() & key & f'block_start >= \"{pre_social_start}\"' & f'block_start <= \"{pre_social_end}\"').fetch(format=\"frame\")\n",
    "    block_subject_patch_data_pre_social.reset_index(level=[\"experiment_name\"], drop=True, inplace=True) \n",
    "    block_subject_patch_data_pre_social.reset_index(inplace=True)\n",
    "    \n",
    "    block_subject_patch_data_social = (BlockSubjectAnalysis.Patch() & key & f'block_start >= \"{social_start}\"' & f'block_start <= \"{social_end}\"').fetch(format=\"frame\")\n",
    "    block_subject_patch_data_social.reset_index(level=[\"experiment_name\"], drop=True, inplace=True)\n",
    "    block_subject_patch_data_social.reset_index(inplace=True)\n",
    "    \n",
    "    block_subject_patch_data_post_social = (BlockSubjectAnalysis.Patch() & key & f'block_start >= \"{post_social_start}\"' & f'block_start <= \"{post_social_end}\"').fetch(format=\"frame\")\n",
    "    block_subject_patch_data_post_social.reset_index(level=[\"experiment_name\"], drop=True, inplace=True)\n",
    "    block_subject_patch_data_post_social.reset_index(inplace=True)\n",
    "    \n",
    "    return patch_info, block_subject_patch_data_pre_social, block_subject_patch_data_social, block_subject_patch_data_post_social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    {\"name\": \"social0.2-aeon3\", \"pre_social_start\": '2024-01-31 10:00:00', \"pre_social_end\": '2024-02-08 15:00:00', \"social_start\": '2024-02-09 16:00:00', \"social_end\": '2024-02-23 13:00:00', \"post_social_start\": '2024-02-25 16:00:00', \"post_social_end\": '2024-03-02 14:00:00'},\n",
    "    {\"name\": \"social0.2-aeon4\", \"pre_social_start\": '2024-01-31 09:00:00', \"pre_social_end\": '2024-02-08 15:00:00', \"social_start\": '2024-02-09 16:00:00', \"social_end\": '2024-02-23 12:00:00', \"post_social_start\": '2024-02-25 16:00:00', \"post_social_end\": '2024-03-02 13:00:00'},\n",
    "    {\"name\": \"social0.3-aeon3\", \"pre_social_start\": '2024-06-08 18:00:00', \"pre_social_end\": '2024-06-17 13:00:00', \"social_start\": '2024-06-25 10:00:00', \"social_end\": '2024-07-06 13:00:00', \"post_social_start\": '2024-07-07 15:00:00', \"post_social_end\": '2024-07-14 14:00:00'},\n",
    "    {\"name\": \"social0.3-aeon4\", \"pre_social_start\": '2024-06-08 18:00:00', \"pre_social_end\": '2024-06-17 14:00:00', \"social_start\": '2024-06-19 11:00:00', \"social_end\": '2024-07-03 14:00:00', \"post_social_start\": '2024-07-04 10:00:00', \"post_social_end\": '2024-07-13 12:00:00'},\n",
    "    {\"name\": \"social0.4-aeon3\", \"pre_social_start\": '2024-08-16 16:00:00', \"pre_social_end\": '2024-08-24 10:00:00', \"social_start\": '2024-08-28 10:00:00', \"social_end\": '2024-09-09 13:00:00', \"post_social_start\": '2024-09-09 17:00:00', \"post_social_end\": '2024-09-22 16:00:00'},\n",
    "    {\"name\": \"social0.4-aeon4\", \"pre_social_start\": '2024-08-16 14:00:00', \"pre_social_end\": '2024-08-24 10:00:00', \"social_start\": '2024-08-28 09:00:00', \"social_end\": '2024-09-09 01:00:00', \"post_social_start\": '2024-09-09 14:00:00', \"post_social_end\": '2024-09-22 16:00:00'}\n",
    "]\n",
    "\n",
    "patch_info_dict = {}\n",
    "block_subject_patch_data_social_first_half_dict = {}\n",
    "block_subject_patch_data_social_dict = {}\n",
    "block_subject_patch_data_post_social_dict = {}\n",
    "\n",
    "for exp in experiments:\n",
    "    key = {\"experiment_name\": exp[\"name\"]}\n",
    "    pre_social_start = datetime.strptime(exp[\"pre_social_start\"], '%Y-%m-%d %H:%M:%S')\n",
    "    pre_social_end = datetime.strptime(exp[\"pre_social_end\"], '%Y-%m-%d %H:%M:%S')\n",
    "    social_start = datetime.strptime(exp[\"social_start\"], '%Y-%m-%d %H:%M:%S')\n",
    "    social_end = datetime.strptime(exp[\"social_end\"], '%Y-%m-%d %H:%M:%S')\n",
    "    post_social_start = datetime.strptime(exp[\"post_social_start\"], '%Y-%m-%d %H:%M:%S')\n",
    "    post_social_end = datetime.strptime(exp[\"post_social_end\"], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    patch_info, _, block_subject_patch_data_social, block_subject_patch_data_post_social = load_experiment_data(\n",
    "        key, pre_social_start, pre_social_end, social_start, social_end, post_social_start, post_social_end\n",
    "    )\n",
    "\n",
    "    # Drop rows where patch_name contains 'PatchDummy'\n",
    "    block_subject_patch_data_social = block_subject_patch_data_social[~block_subject_patch_data_social['patch_name'].str.contains('PatchDummy')]\n",
    "    block_subject_patch_data_post_social = block_subject_patch_data_post_social[~block_subject_patch_data_post_social['patch_name'].str.contains('PatchDummy')]\n",
    "\n",
    "    # Check post-social data only has one subject being tracked per block\n",
    "    max_num_subjects = block_subject_patch_data_post_social.groupby('block_start')['subject_name'].nunique().max()\n",
    "    if max_num_subjects > 1:\n",
    "        warnings.warn(f\"Post social data for {exp['name']} has more than one subject being tracked per block. Data needs to be fixed or cleaned.\")\n",
    "\n",
    "    patch_info_dict[exp[\"name\"]] = patch_info\n",
    "    social_midpoint = social_start + (social_end - social_start) / 2\n",
    "    block_subject_patch_data_social_first_half_dict[exp[\"name\"]] = block_subject_patch_data_social[block_subject_patch_data_social['block_start'] < social_midpoint]\n",
    "    block_subject_patch_data_social_dict[exp[\"name\"]] = block_subject_patch_data_social\n",
    "    block_subject_patch_data_post_social_dict[exp[\"name\"]] = block_subject_patch_data_post_social\n",
    "\n",
    "# Combine loaded data\n",
    "block_subject_patch_data_social_first_half_combined = pd.concat(block_subject_patch_data_social_first_half_dict.values()) # TODO: take even less? Match the num of days in post social?\n",
    "block_subject_patch_data_social_combined = pd.concat(block_subject_patch_data_social_dict.values())\n",
    "block_subject_patch_data_post_social_combined = pd.concat(block_subject_patch_data_post_social_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create block plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Wheel distance spun per block, averaged by the number of mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot.\"\"\"\n",
    "block_subject_patch_data_social_combined['final_wheel_cumsum'] = block_subject_patch_data_social_combined['wheel_cumsum_distance_travelled'].apply(lambda x: x[-1] if isinstance(x, np.ndarray) and len(x) > 0 else 0)\n",
    "wheel_total_dist_averaged_social = block_subject_patch_data_social_combined.groupby('block_start')['final_wheel_cumsum'].sum() / 2\n",
    "wheel_total_dist_averaged_social = wheel_total_dist_averaged_social.reset_index()\n",
    "\n",
    "block_subject_patch_data_post_social_combined['final_wheel_cumsum'] = block_subject_patch_data_post_social_combined['wheel_cumsum_distance_travelled'].apply(lambda x: x[-1] if isinstance(x, np.ndarray) and len(x) > 0 else 0)\n",
    "wheel_total_dist_averaged_post_social = block_subject_patch_data_post_social_combined.groupby('block_start')['final_wheel_cumsum'].sum().reset_index()\n",
    "\n",
    "wheel_total_dist_averaged_social['condition'] = 'social'\n",
    "wheel_total_dist_averaged_post_social['condition'] = 'post_social'\n",
    "wheel_total_dist_averaged = pd.concat([wheel_total_dist_averaged_social, wheel_total_dist_averaged_post_social])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig = px.box(\n",
    "    wheel_total_dist_averaged,\n",
    "    x=\"condition\",\n",
    "    y=\"final_wheel_cumsum\",\n",
    "    points=\"all\",\n",
    "    title=\"Wheel Distance Spun Per Block Averaged By Number Of Subjects\",\n",
    "    labels={\"final_wheel_cumsum\": \"Wheel Distance Spun Per Block (cm)\"},\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Number of patch switches by each mouse per block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_patch_probabilities(\n",
    "    df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute patch probabilities based on block and subject data.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing block, subject, pellet, and patch data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the probabilities for each patch.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Precompute unique block-subject groups\n",
    "    grouped_data = df.groupby(['block_start', 'subject_name'])\n",
    "\n",
    "    for (block_start, subject_name), block_data in grouped_data:\n",
    "        # Process pellet timestamps once\n",
    "        pellet_timestamps = np.sort(np.unique([ts for sublist in block_data['pellet_timestamps'] for ts in sublist]))\n",
    "        if len(pellet_timestamps) < 2:\n",
    "            continue\n",
    "\n",
    "        # Create pellet intervals DataFrame\n",
    "        intervals_df = pd.DataFrame({\n",
    "            'interval_start': pellet_timestamps[:-1],\n",
    "            'interval_end': pellet_timestamps[1:],\n",
    "            'pellet_number': np.arange(1, len(pellet_timestamps))\n",
    "        })\n",
    "\n",
    "        # Prepare a dict to hold in_patch_timestamps for each patch\n",
    "        patches_data = {}\n",
    "        for patch in block_data['patch_name'].unique():\n",
    "            patch_data = block_data[block_data['patch_name'] == patch]\n",
    "            if patch_data.shape[0] != 1:\n",
    "                raise ValueError(\"More than one row per block start, subject, patch combination.\")\n",
    "            in_patch_timestamps = np.sort(patch_data.iloc[0]['in_patch_timestamps'])\n",
    "            patches_data[patch] = in_patch_timestamps\n",
    "\n",
    "        # Initialize a DataFrame to store counts per patch\n",
    "        counts_df = intervals_df[['pellet_number']].copy()\n",
    "\n",
    "        # For each patch, compute counts within each interval using numpy searchsorted\n",
    "        for patch, in_patch_ts in patches_data.items():\n",
    "            counts = np.zeros(len(intervals_df), dtype=int)\n",
    "            if len(in_patch_ts) > 0:\n",
    "                idx_start = np.searchsorted(in_patch_ts, intervals_df['interval_start'].values, side='left')\n",
    "                idx_end = np.searchsorted(in_patch_ts, intervals_df['interval_end'].values, side='right')\n",
    "                counts = idx_end - idx_start\n",
    "            counts_df[f'count_in_{patch}'] = counts\n",
    "\n",
    "        # Compute total counts per interval\n",
    "        counts_df['total_counts'] = counts_df.filter(like='count_in_').sum(axis=1)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        counts_df['total_counts'] = counts_df['total_counts'].replace(0, np.nan)\n",
    "\n",
    "        # Compute probabilities per interval\n",
    "        for idx, row in counts_df.iterrows():\n",
    "            pellet_number = row['pellet_number']\n",
    "            row_data = {\n",
    "                'block_start': block_start,\n",
    "                'subject_name': subject_name,\n",
    "                'pellet_number': pellet_number\n",
    "            }\n",
    "            ts_in_patches = {patch: row[f'count_in_{patch}'] for patch in patches_data.keys()}\n",
    "            ts_in_patches_total = row['total_counts']\n",
    "            if pd.isna(ts_in_patches_total):\n",
    "                prob = {patch: 0 for patch in ts_in_patches.keys()}\n",
    "            else:\n",
    "                prob = {patch: ts_in_patches[patch] / ts_in_patches_total for patch in ts_in_patches.keys()}\n",
    "            row_data.update({f'prob_in_{patch}': prob[patch] for patch in patches_data.keys()})\n",
    "            results.append(row_data)\n",
    "\n",
    "    # Create final DataFrame\n",
    "    prob_per_patch = pd.DataFrame(results)\n",
    "    return prob_per_patch\n",
    "\n",
    "def extract_hard_patch_probabilities(\n",
    "    prob_per_patch: pd.DataFrame, \n",
    "    patch_info: List[Dict[str, Any]], \n",
    "    patch_rate: float = 0.002\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the probabilities for hard patches where the patch rate matches a specified value.\n",
    "\n",
    "    Args:\n",
    "        prob_per_patch (pd.DataFrame): DataFrame containing probabilities per patch.\n",
    "        patch_info (List[Dict[str, Any]]): List of dictionaries with patch information.\n",
    "        patch_rate (float): The patch rate to filter by. Default is 0.002.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the probabilities for each hard patch.\n",
    "    \"\"\"\n",
    "    # Filter the hard patches based on the patch_rate\n",
    "    hard_patches = [patch_dict for patch_dict in patch_info if patch_dict['patch_rate'] == patch_rate]\n",
    "    \n",
    "    results = []\n",
    "    for hard_patch in hard_patches:\n",
    "        block_start = hard_patch['block_start']\n",
    "        patch_name = hard_patch['patch_name']\n",
    "        \n",
    "        # Extract the hard patch data\n",
    "        hard_patch_data = prob_per_patch.loc[\n",
    "            prob_per_patch['block_start'] == block_start, \n",
    "            ['block_start', 'subject_name', 'pellet_number', f'prob_in_{patch_name}']\n",
    "        ]\n",
    "        \n",
    "        # Rename the column for hard patch probability\n",
    "        hard_patch_data = hard_patch_data.rename(columns={f'prob_in_{patch_name}': 'prob_in_hard_patch'})\n",
    "        \n",
    "        # Append the result to the list\n",
    "        results.append(hard_patch_data)\n",
    "        \n",
    "    # Concatenate all results into a single DataFrame\n",
    "    prob_hard_patch = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "    return prob_hard_patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_per_patch_social_first_half_dict = {}\n",
    "prob_per_patch_social_dict = {}\n",
    "prob_per_patch_post_social_dict = {}\n",
    "prob_hard_patch_social_first_half_dict = {}\n",
    "prob_hard_patch_social_dict = {}\n",
    "prob_hard_patch_post_social_dict = {}\n",
    "prob_hard_patch_mean_social_first_half_dict = {}\n",
    "prob_hard_patch_mean_social_dict = {}\n",
    "prob_hard_patch_mean_post_social_dict = {}\n",
    "\n",
    "for exp in experiments:\n",
    "    exp_name = exp[\"name\"]\n",
    "    block_subject_patch_data_social_first_half = block_subject_patch_data_social_first_half_dict[exp_name]\n",
    "    block_subject_patch_data_social = block_subject_patch_data_social_dict[exp_name]\n",
    "    block_subject_patch_data_post_social = block_subject_patch_data_post_social_dict[exp_name]\n",
    "    patch_info = patch_info_dict[exp_name]\n",
    "\n",
    "    # Compute patch probabilities\n",
    "    prob_per_patch_social_first_half_dict[exp_name] = compute_patch_probabilities(block_subject_patch_data_social_first_half)\n",
    "    prob_per_patch_social_dict[exp_name] = compute_patch_probabilities(block_subject_patch_data_social)\n",
    "    prob_per_patch_post_social_dict[exp_name] = compute_patch_probabilities(block_subject_patch_data_post_social)\n",
    "\n",
    "    # Extract hard patch probabilities\n",
    "    prob_hard_patch_social_first_half_dict[exp_name] = extract_hard_patch_probabilities(prob_per_patch_social_first_half_dict[exp_name], patch_info)\n",
    "    prob_hard_patch_social_dict[exp_name] = extract_hard_patch_probabilities(prob_per_patch_social_dict[exp_name], patch_info)\n",
    "    prob_hard_patch_post_social_dict[exp_name] = extract_hard_patch_probabilities(prob_per_patch_post_social_dict[exp_name], patch_info)\n",
    "\n",
    "    # Calculate the mean hard patch probability per pellet number\n",
    "    prob_hard_patch_mean_social_first_half_dict[exp_name] = prob_hard_patch_social_first_half_dict[exp_name].groupby('pellet_number').mean().reset_index()\n",
    "    prob_hard_patch_mean_social_dict[exp_name] = prob_hard_patch_social_dict[exp_name].groupby('pellet_number').mean().reset_index()\n",
    "    prob_hard_patch_mean_post_social_dict[exp_name] = prob_hard_patch_post_social_dict[exp_name].groupby('pellet_number').mean().reset_index()\n",
    "\n",
    "# Combine the results\n",
    "prob_hard_patch_social_first_half_combined = pd.concat(prob_hard_patch_social_first_half_dict.values())\n",
    "prob_hard_patch_social_combined = pd.concat(prob_hard_patch_social_dict.values())\n",
    "prob_hard_patch_post_social_combined = pd.concat(prob_hard_patch_post_social_dict.values())\n",
    "\n",
    "prob_hard_patch_mean_social_first_half_combined = prob_hard_patch_social_first_half_combined.groupby('pellet_number').mean().reset_index()\n",
    "prob_hard_patch_mean_social_combined = prob_hard_patch_social_combined.groupby('pellet_number').mean().reset_index()\n",
    "prob_hard_patch_mean_post_social_combined = prob_hard_patch_post_social_combined.groupby('pellet_number').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_patch_probabilities(\n",
    "    data: pd.DataFrame, \n",
    "    label: str\n",
    ") -> Tuple[sm.OLS, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Analyze patch probabilities using a linear regression model.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame containing the data to analyze.\n",
    "        label (str): Label for the analysis.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - model (sm.OLS): The fitted linear regression model.\n",
    "            - y_pred (np.ndarray): The predicted values from the model.\n",
    "    \"\"\"\n",
    "    # Prepare the data for statsmodels (add a constant for the intercept)\n",
    "    X = np.array(data['pellet_number'][0:35])\n",
    "    y = np.array(data['prob_in_hard_patch'][0:35])\n",
    "    # Add a constant to the independent variable X to calculate the intercept\n",
    "    X_with_constant = sm.add_constant(X)\n",
    "    # Fit the model using statsmodels\n",
    "    model = sm.OLS(y, X_with_constant).fit()\n",
    "    y_pred = model.predict(X_with_constant)\n",
    "    # Get the p-value for the slope (it's the second value in pvalues)\n",
    "    p_value = model.pvalues[1]\n",
    "    print(f\"P-value for the {label} slope: {p_value}\")\n",
    "    # Print full statistical summary\n",
    "    print(f\"{label} model summary: \", model.summary())\n",
    "    return model, y_pred\n",
    "\n",
    "model_social_first_half, y_pred_social_first_half = analyze_patch_probabilities(prob_hard_patch_mean_social_first_half_combined, \"social first half\")\n",
    "model_social, y_pred_social = analyze_patch_probabilities(prob_hard_patch_mean_social_combined, \"social\")\n",
    "model_post_social, y_pred_post_social = analyze_patch_probabilities(prob_hard_patch_mean_post_social_combined, \"post-social\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=prob_hard_patch_mean_social_first_half_combined['pellet_number'], #[0:35],\n",
    "    y=prob_hard_patch_mean_social_first_half_combined['prob_in_hard_patch'], #[0:35],\n",
    "    mode='lines',\n",
    "    name='First Half of Social Data',\n",
    "    marker=dict(color='blue')\n",
    "))\n",
    "    \n",
    "fig.add_trace(go.Scatter(\n",
    "    x=prob_hard_patch_mean_social_combined['pellet_number'], #[0:35],\n",
    "    y=prob_hard_patch_mean_social_combined['prob_in_hard_patch'], #[0:35],\n",
    "    mode='lines',\n",
    "    name='Social Data',\n",
    "    marker=dict(color='red')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=prob_hard_patch_mean_post_social_combined['pellet_number'], #[0:35],\n",
    "    y=prob_hard_patch_mean_post_social_combined['prob_in_hard_patch'], #[0:35],\n",
    "    mode='lines',\n",
    "    name='Post Social Data',\n",
    "    marker=dict(color='#00CC96')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=prob_hard_patch_mean_social_first_half_combined['pellet_number'][0:35],\n",
    "    y=y_pred_social_first_half,\n",
    "    mode='lines',\n",
    "    name='Social First Half Linear Regression Line',\n",
    "    line=dict(dash='dash'),\n",
    "    marker=dict(color='blue')  # Optional: to make the regression line dashed\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=prob_hard_patch_mean_social_combined['pellet_number'][0:35],\n",
    "    y=y_pred_social,\n",
    "    mode='lines',\n",
    "    name='Social Linear Regression Line',\n",
    "    line=dict(dash='dash'),\n",
    "    marker=dict(color='red')  # Optional: to make the regression line dashed\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=prob_hard_patch_mean_social_combined['pellet_number'][0:35],\n",
    "    y=y_pred_post_social,\n",
    "    mode='lines',\n",
    "    name='Post Social Linear Regression Line',\n",
    "    line=dict(dash='dash'),  # Optional: to make the regression line dashed\n",
    "    marker=dict(color='#00CC96')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Probability of being in hard patch over time',\n",
    "    xaxis_title='Pellet number in block',\n",
    "    yaxis_title='Hard patch probability'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the figure as an SVG file\n",
    "# fig.write_image(\"hard_patch_probability.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of rows and columns for the subplot grid\n",
    "num_experiments = len(experiments)\n",
    "num_cols = 2  \n",
    "num_rows = (num_experiments + num_cols - 1) // num_cols  \n",
    "\n",
    "# Create a subplot grid\n",
    "fig = make_subplots(rows=num_rows, cols=num_cols, subplot_titles=[exp[\"name\"] for exp in experiments])\n",
    "\n",
    "# Iterate over each experiment and add a plot to the grid\n",
    "for i, exp in enumerate(experiments):\n",
    "    exp_name = exp[\"name\"]\n",
    "    row = (i // num_cols) + 1\n",
    "    col = (i % num_cols) + 1\n",
    "\n",
    "    # Add the plot to the grid\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=prob_hard_patch_mean_social_first_half_dict[exp_name]['pellet_number'][0:35], \n",
    "        y=prob_hard_patch_mean_social_first_half_dict[exp_name]['prob_in_hard_patch'][0:35],\n",
    "        mode='lines', \n",
    "        name='First Half of Social Data',\n",
    "        marker=dict(color='blue'),\n",
    "        showlegend=(i == 0)),\n",
    "    row=row, col=col)\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=prob_hard_patch_mean_social_dict[exp_name]['pellet_number'][0:35], \n",
    "        y=prob_hard_patch_mean_social_dict[exp_name]['prob_in_hard_patch'][0:35],\n",
    "        mode='lines', \n",
    "        name='Social Data',\n",
    "        marker=dict(color='red'),\n",
    "        showlegend=(i == 0)),\n",
    "    row=row, col=col)\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=prob_hard_patch_mean_post_social_dict[exp_name]['pellet_number'][0:35], \n",
    "        y=prob_hard_patch_mean_post_social_dict[exp_name]['prob_in_hard_patch'][0:35],\n",
    "        mode='lines', \n",
    "        name='Post Social Data',\n",
    "        marker=dict(color='#00CC96'),\n",
    "        showlegend=(i == 0)),\n",
    "    row=row, col=col)\n",
    "\n",
    "\n",
    "fig.update_layout(height=800, width=1000)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "aeon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
